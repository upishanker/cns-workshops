{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcd6008",
   "metadata": {},
   "source": [
    "# 2. Overfitting & Validation\n",
    "\n",
    "This notebook covers overfitting, underfitting, and cross-validation techniques for model evaluation.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f37cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load ABIDE data\n",
    "data = pd.read_csv('./data/abide2.tsv', sep='\\t')\n",
    "X_all = data.filter(like='fs')\n",
    "y = data['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d878a92",
   "metadata": {},
   "source": [
    "## Overfitting vs. Underfitting\n",
    "\n",
    "**Overfitting**: Model fits training data too closely, capturing noise instead of signal\n",
    "- High training performance, poor test performance\n",
    "- Model is too flexible/complex\n",
    "\n",
    "**Underfitting**: Model is too simple to capture patterns\n",
    "- Poor training and test performance\n",
    "- Model lacks flexibility\n",
    "\n",
    "### Toy Example: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb614f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with quadratic relationship\n",
    "np.random.seed(10)\n",
    "x = np.random.normal(size=30)\n",
    "y_syn = (0.7 * x) ** 2 + 0.1 * x + np.random.normal(10, 0.5, size=30)\n",
    "\n",
    "# Helper function to create polynomial pipelines\n",
    "def make_pipeline(degree=1):\n",
    "    return Pipeline([\n",
    "        (\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"linear_regression\", LinearRegression())\n",
    "    ])\n",
    "\n",
    "# Fit models with different polynomial degrees\n",
    "degrees = [1, 2, 10]\n",
    "x_range = np.linspace(x.min(), x.max(), 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "titles = ['Underfitting (degree=1)', 'Good Fit (degree=2)', 'Overfitting (degree=10)']\n",
    "\n",
    "for ax, degree, title in zip(axes, degrees, titles):\n",
    "    model = make_pipeline(degree)\n",
    "    model.fit(x[:, None], y_syn)\n",
    "    \n",
    "    y_fit = model.predict(x_range[:, None])\n",
    "    mse = mean_squared_error(y_syn, model.predict(x[:, None]))\n",
    "    \n",
    "    ax.scatter(x, y_syn, s=50, alpha=0.7)\n",
    "    ax.plot(x_range, y_fit, 'r-', lw=2)\n",
    "    ax.set_title(f\"{title}\\nMSE = {mse:.2f}\")\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600937a",
   "metadata": {},
   "source": [
    "**Key insight**: The 10th-degree polynomial has the lowest training error but clearly overfits!\n",
    "\n",
    "## Cross-Validation: Train/Test Split\n",
    "\n",
    "To detect overfitting, we must evaluate on **independent test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3991dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 50% train, 50% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y, train_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eaca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model using ALL features (will overfit!)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both train and test sets\n",
    "r2_train = model.score(X_train, y_train)\n",
    "r2_test = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"R² on training set: {r2_train:.3f}\")\n",
    "print(f\"R² on test set: {r2_test:.3f}\")\n",
    "print(f\"\\n⚠️ Difference: {r2_train - r2_test:.3f} indicates severe overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fe82f",
   "metadata": {},
   "source": [
    "### Solution: Use Fewer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28198609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample only 200 features\n",
    "X_small = X_all.sample(200, axis='columns', random_state=99)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y, train_size=0.5, random_state=99\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2_train = model.score(X_train, y_train)\n",
    "r2_test = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"R² on training set: {r2_train:.3f}\")\n",
    "print(f\"R² on test set: {r2_test:.3f}\")\n",
    "print(f\"\\n✓ Much better! Model generalizes to new data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a8cf3",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "Problem with train/test split: wastes data (only 50% used for training).\n",
    "\n",
    "**Solution**: K-fold cross-validation\n",
    "1. Split data into K folds\n",
    "2. For each fold: train on K-1 folds, test on remaining fold\n",
    "3. Average performance across all K folds\n",
    "\n",
    "Benefits:\n",
    "- Uses all data for both training and testing\n",
    "- More stable performance estimates\n",
    "- Reduces variance in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold cross-validation\n",
    "K = 5\n",
    "model = LinearRegression()\n",
    "\n",
    "# cross_val_score returns array of scores (one per fold)\n",
    "cv_scores = cross_val_score(model, X_small, y, cv=K, scoring='r2')\n",
    "\n",
    "print(\"Individual fold scores:\", cv_scores.round(3))\n",
    "print(f\"\\nMean cross-validated R²: {cv_scores.mean():.3f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7536f",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Visualize model performance as a function of training set size to diagnose overfitting/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Sample 100 features\n",
    "X = X_all.sample(100, axis=1, random_state=100)\n",
    "\n",
    "# Training sizes to evaluate\n",
    "train_sizes = [100, 200, 400, 800]\n",
    "\n",
    "# Compute learning curve\n",
    "sizes, train_scores, test_scores = learning_curve(\n",
    "    LinearRegression(), X, y, \n",
    "    train_sizes=train_sizes, \n",
    "    cv=5, \n",
    "    shuffle=True,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Calculate means\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, train_mean, 'o-', label='Training', linewidth=2, markersize=8)\n",
    "plt.plot(sizes, test_mean, 'o-', label='Test (Cross-Validation)', linewidth=2, markersize=8)\n",
    "plt.fill_between(sizes, 0, train_mean, alpha=0.1)\n",
    "plt.fill_between(sizes, 0, test_mean, alpha=0.1)\n",
    "plt.xlabel('Training Set Size', fontsize=12)\n",
    "plt.ylabel('R²', fontsize=12)\n",
    "plt.title('Learning Curve: Performance vs. Training Size', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Large gap between train/test curves = overfitting\")\n",
    "print(\"- Test curve improves with more data = model benefits from larger samples\")\n",
    "print(\"- Curves haven't converged = could benefit from even more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e4c82",
   "metadata": {},
   "source": [
    "## Model Complexity vs. Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different feature set sizes\n",
    "feature_counts = [5, 30, 100]\n",
    "train_sizes = [100, 200, 400, 800]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, n_feat in zip(axes, feature_counts):\n",
    "    X_subset = X_all.sample(n_feat, axis=1, random_state=99)\n",
    "    \n",
    "    sizes, train_scores, test_scores = learning_curve(\n",
    "        LinearRegression(), X_subset, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=5,\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean = test_scores.mean(axis=1)\n",
    "    \n",
    "    ax.plot(sizes, train_mean, 'o-', label='Train', linewidth=2)\n",
    "    ax.plot(sizes, test_mean, 'o-', label='Test', linewidth=2)\n",
    "    ax.set_title(f'{n_feat} Features', fontsize=12)\n",
    "    ax.set_xlabel('Training Size')\n",
    "    ax.set_ylabel('R²')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Model complexity must match dataset size!\")\n",
    "print(\"- Small samples: simpler models perform better\")\n",
    "print(\"- Large samples: complex models can leverage more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6eaf54",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Critical Concepts:**\n",
    "\n",
    "1. **Always evaluate on independent test data** to detect overfitting\n",
    "2. **K-fold cross-validation** provides robust performance estimates\n",
    "3. **Learning curves** diagnose overfitting and data needs\n",
    "4. **Balance model complexity with dataset size**\n",
    "\n",
    "**Rules of Thumb:**\n",
    "- Large train-test gap → overfitting (reduce complexity or add data)\n",
    "- Poor performance on both → underfitting (increase complexity)\n",
    "- Test performance improves with more data → collect more samples\n",
    "\n",
    "**Next**: Learn regularization techniques to prevent overfitting!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
