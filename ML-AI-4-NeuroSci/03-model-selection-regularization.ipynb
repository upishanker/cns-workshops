{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecd23c6",
   "metadata": {},
   "source": [
    "# 3. Model Selection & Regularization\n",
    "\n",
    "This notebook covers regularization techniques to prevent overfitting and improve predictions.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4935c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_score, validation_curve\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('./data/abide2.tsv', sep='\\t')\n",
    "features = data.filter(like='fs')\n",
    "y = data['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df625679",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "**Bias**: Systematic error from incorrect assumptions\n",
    "- High bias → underfitting (too simple)\n",
    "- Low bias → captures complex patterns\n",
    "\n",
    "**Variance**: Sensitivity to training data fluctuations\n",
    "- High variance → overfitting (too flexible)\n",
    "- Low variance → stable predictions\n",
    "\n",
    "**Key insight**: Reducing variance often requires accepting some bias (and vice versa).\n",
    "\n",
    "## Regularization Principle\n",
    "\n",
    "**Regularization** adds constraints to models based on prior knowledge:\n",
    "- Introduces beneficial bias\n",
    "- Reduces variance\n",
    "- Prevents overfitting\n",
    "- Improves generalization to new data\n",
    "\n",
    "## Penalized Regression\n",
    "\n",
    "### Ordinary Least Squares (OLS)\n",
    "$$Cost = \\sum_{i=1}^N (y_i - \\sum_{j=1}^P \\beta_j x_{ij})^2$$\n",
    "\n",
    "### Lasso Regression (L1 penalty)\n",
    "$$Cost = \\sum_{i=1}^N (y_i - \\sum_{j=1}^P \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^P |\\beta_j|$$\n",
    "\n",
    "- Shrinks coefficients toward zero\n",
    "- Sets many coefficients exactly to zero → **feature selection**\n",
    "- Best when few features are truly important\n",
    "\n",
    "### Ridge Regression (L2 penalty)\n",
    "$$Cost = \\sum_{i=1}^N (y_i - \\sum_{j=1}^P \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^P \\beta_j^2$$\n",
    "\n",
    "- Shrinks coefficients toward zero\n",
    "- Never sets coefficients exactly to zero\n",
    "- Best when many features contribute\n",
    "\n",
    "**Parameter λ (alpha)**: Controls penalty strength\n",
    "- λ = 0: no penalty (equivalent to OLS)\n",
    "- Large λ: strong penalty (more regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a53022",
   "metadata": {},
   "source": [
    "## Visualizing Coefficient Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and standardize features\n",
    "N_FEATURES = 200\n",
    "X = features.sample(N_FEATURES, axis=1, random_state=99)\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Define helper function to plot coefficient paths\n",
    "def plot_coef_path(estimator, X, y, alpha_range, title):\n",
    "    coefs = np.zeros((X.shape[1], len(alpha_range)))\n",
    "    for i, alpha in enumerate(alpha_range):\n",
    "        model = estimator(alpha=alpha, max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        coefs[:, i] = model.coef_\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(alpha_range, coefs.T, alpha=0.7)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Penalty (α)', fontsize=12)\n",
    "    plt.ylabel('Coefficient Value', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso coefficient paths\n",
    "alpha_lasso = np.logspace(-3, 1, 100)\n",
    "plot_coef_path(Lasso, X_scaled, y, alpha_lasso, \n",
    "               'Lasso: Coefficients Shrink to Zero (Sparse Solution)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge coefficient paths\n",
    "alpha_ridge = np.logspace(-5, 5, 100)\n",
    "plot_coef_path(Ridge, X_scaled, y, alpha_ridge,\n",
    "               'Ridge: Coefficients Shrink but Never Reach Zero (Dense Solution)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a803dba",
   "metadata": {},
   "source": [
    "## Tuning the Penalty Parameter\n",
    "\n",
    "How do we choose the optimal penalty? Use **validation curves** to find the α that maximizes test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ea051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot validation curves\n",
    "def plot_validation_curve(model, X, y, param_range, model_name, baseline_r2=None):\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        model, X, y,\n",
    "        param_name='alpha',\n",
    "        param_range=param_range,\n",
    "        cv=5,\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean = test_scores.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(param_range, train_mean, 'o-', label='Training', linewidth=2)\n",
    "    plt.plot(param_range, test_mean, 'o-', label='Test (CV)', linewidth=2)\n",
    "    \n",
    "    if baseline_r2 is not None:\n",
    "        plt.axhline(baseline_r2, linestyle='--', color='gray', \n",
    "                    linewidth=2, label='OLS (baseline)')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Penalty (α)', fontsize=12)\n",
    "    plt.ylabel('R²', fontsize=12)\n",
    "    plt.title(f'{model_name} Validation Curve', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Find optimal alpha\n",
    "    best_idx = test_mean.argmax()\n",
    "    best_alpha = param_range[best_idx]\n",
    "    best_score = test_mean[best_idx]\n",
    "    \n",
    "    plt.axvline(best_alpha, linestyle=':', color='red', alpha=0.7)\n",
    "    plt.text(best_alpha, 0.05, f'Optimal α = {best_alpha:.4f}\\nR² = {best_score:.3f}',\n",
    "             fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return best_alpha, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OLS baseline for comparison\n",
    "ols_r2 = cross_val_score(LinearRegression(), X_scaled, y, cv=5, scoring='r2').mean()\n",
    "print(f\"OLS baseline R²: {ols_r2:.3f}\\n\")\n",
    "\n",
    "# Lasso validation curve\n",
    "alpha_range_lasso = np.logspace(-3, 1, 30)\n",
    "best_lasso_alpha, best_lasso_r2 = plot_validation_curve(\n",
    "    Lasso(max_iter=5000), X_scaled, y, alpha_range_lasso, 'Lasso', ols_r2\n",
    ")\n",
    "\n",
    "print(f\"\\nLasso: Best α = {best_lasso_alpha:.4f}, R² = {best_lasso_r2:.3f}\")\n",
    "print(f\"Improvement over OLS: {best_lasso_r2 - ols_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e752d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge validation curve\n",
    "alpha_range_ridge = np.logspace(0, 7, 50)\n",
    "best_ridge_alpha, best_ridge_r2 = plot_validation_curve(\n",
    "    Ridge(), X_scaled, y, alpha_range_ridge, 'Ridge', ols_r2\n",
    ")\n",
    "\n",
    "print(f\"\\nRidge: Best α = {best_ridge_alpha:.4f}, R² = {best_ridge_r2:.3f}\")\n",
    "print(f\"Improvement over OLS: {best_ridge_r2 - ols_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46a220",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "Let's compare OLS, Lasso, and Ridge using their optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceacf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('OLS', LinearRegression()),\n",
    "    ('Lasso', Lasso(alpha=best_lasso_alpha, max_iter=5000)),\n",
    "    ('Ridge', Ridge(alpha=best_ridge_alpha))\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean R²': scores.mean(),\n",
    "        'Std R²': scores.std()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(results_df['Model'], results_df['Mean R²'], yerr=results_df['Std R²'],\n",
    "        capsize=5, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Cross-Validated R²', fontsize=12)\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "plt.ylim(0, max(results_df['Mean R²']) * 1.2)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc92cc1",
   "metadata": {},
   "source": [
    "## Beyond Linear Models: Random Forests\n",
    "\n",
    "**Random Forests** are ensembles of decision trees:\n",
    "- Very flexible (can capture non-linear patterns)\n",
    "- Handle high-dimensional data well\n",
    "- Provide feature importances\n",
    "- Can overfit if not properly tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Select 50 best features (reduces computational cost)\n",
    "selector = SelectKBest(f_regression, k=50)\n",
    "X_selected = selector.fit_transform(features, y)\n",
    "\n",
    "# Train random forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Random Forest R²: {rf_scores.mean():.3f} ± {rf_scores.std():.3f}\")\n",
    "\n",
    "# Compare with Ridge on same features\n",
    "ridge = Ridge(alpha=100)\n",
    "ridge_scores = cross_val_score(ridge, scale(X_selected), y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Ridge R²: {ridge_scores.mean():.3f} ± {ridge_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e29f6",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit random forest on 30 random features for interpretability\n",
    "X_subset = features.sample(30, axis=1, random_state=42)\n",
    "rf_interp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_interp.fit(X_subset, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf_interp.feature_importances_, index=X_subset.columns)\n",
    "top_features = importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(top_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features.plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Random Forest: Top 10 Features', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c295c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Bias-Variance Tradeoff**: Accept some bias to reduce variance\n",
    "2. **Regularization**: Add constraints to prevent overfitting\n",
    "3. **Lasso**: Sparse solutions (feature selection)\n",
    "4. **Ridge**: Dense solutions (all features contribute)\n",
    "5. **Validation Curves**: Tune hyperparameters systematically\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "- Always compare regularized models to OLS baseline\n",
    "- Use cross-validation to tune penalty parameters\n",
    "- Consider problem structure when choosing regularization:\n",
    "  - Few important features → Lasso\n",
    "  - Many contributing features → Ridge\n",
    "- Try multiple model types (linear, tree-based, etc.)\n",
    "- More data usually beats fancier algorithms\n",
    "\n",
    "**Typical Workflow:**\n",
    "1. Split data (train/test or use CV)\n",
    "2. Try OLS baseline\n",
    "3. Add regularization (Lasso/Ridge)\n",
    "4. Tune hyperparameters via validation curves\n",
    "5. Compare multiple model types\n",
    "6. Select best model based on test performance\n",
    "7. Interpret results cautiously (correlation ≠ causation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
